{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b168694f",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4: Реализация алгоритма Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30555f1d",
   "metadata": {},
   "source": [
    "## Задание:\n",
    "\n",
    "На основе рассмотренного на лекции примера реализуйте алгоритм Policy Iteration для любой среды обучения с подкреплением (кроме рассмотренной на лекции среды Toy Text / Frozen Lake) из библиотеки Gym (или аналогичной библиотеки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee377e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in ./anaconda3/lib/python3.10/site-packages (from gym) (2.0.0)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./anaconda3/lib/python3.10/site-packages (from gym) (1.23.5)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827623 sha256=53209904cda1b89ac05b75ee8035d07d5006d66c0ac087ba37294f347cefc237\n",
      "  Stored in directory: /Users/peterpechenkin/Library/Caches/pip/wheels/ae/5f/67/64914473eb34e9ba89dbc7eefe7e9be8f6673fbc6f0273b29f\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, gym\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8defa8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.5.2-cp310-cp310-macosx_11_0_arm64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2adcafb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пространство состояний:\n",
      "Discrete(48)\n",
      "\n",
      "Пространство действий:\n",
      "Discrete(4)\n",
      "\n",
      "Диапазон наград:\n",
      "(-inf, inf)\n",
      "\n",
      "Вероятности для 0 состояния и 0 действия:\n",
      "[(1.0, 0, -1, False)]\n",
      "\n",
      "Вероятности для 0 состояния:\n",
      "{0: [(1.0, 0, -1, False)],\n",
      " 1: [(1.0, 1, -1, False)],\n",
      " 2: [(1.0, 12, -1, False)],\n",
      " 3: [(1.0, 0, -1, False)]}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "def main():\n",
    "    state, action = 0, 0\n",
    "    env = gym.make(\"CliffWalking-v0\")\n",
    "    print('Пространство состояний:')\n",
    "    pprint(env.observation_space)\n",
    "    print()\n",
    "    print('Пространство действий:')\n",
    "    pprint(env.action_space)\n",
    "    print()\n",
    "    print('Диапазон наград:')\n",
    "    pprint(env.reward_range)\n",
    "    print()\n",
    "    print('Вероятности для 0 состояния и 0 действия:')\n",
    "    pprint(env.P[state][action])\n",
    "    print()\n",
    "    print('Вероятности для 0 состояния:')\n",
    "    pprint(env.P[state])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8541108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стратегия:\n",
      "array([[0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25]])\n",
      "Алгоритм выполнился за 1000 шагов.\n",
      "Стратегия:\n",
      "array([[0.        , 0.5       , 0.5       , 0.        ],\n",
      "       [0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.33333333, 0.        , 0.33333333, 0.33333333],\n",
      "       [0.        , 0.        , 0.5       , 0.5       ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.5       , 0.5       , 0.        ],\n",
      "       [0.        , 0.5       , 0.5       , 0.        ],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.        , 0.5       , 0.5       ],\n",
      "       [0.        , 0.        , 0.5       , 0.5       ],\n",
      "       [0.        , 0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.        , 0.5       , 0.        , 0.5       ],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
      "       [0.        , 0.5       , 0.        , 0.5       ],\n",
      "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.        , 0.33333333, 0.33333333],\n",
      "       [0.5       , 0.        , 0.        , 0.5       ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [1.        , 0.        , 0.        , 0.        ],\n",
      "       [0.5       , 0.5       , 0.        , 0.        ],\n",
      "       [0.33333333, 0.33333333, 0.33333333, 0.        ]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 126\u001b[0m\n\u001b[1;32m    122\u001b[0m     play_agent(agent)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 122\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m agent\u001b[38;5;241m.\u001b[39mprint_policy()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Проигрывание сцены для обученного агента\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43mplay_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 106\u001b[0m, in \u001b[0;36mplay_agent\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m    104\u001b[0m     action \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m    105\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env2\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m--> 106\u001b[0m \u001b[43menv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/envs/toy_text/cliffwalking.py:175\u001b[0m, in \u001b[0;36mCliffWalkingEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/envs/toy_text/cliffwalking.py:262\u001b[0m, in \u001b[0;36mCliffWalkingEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    260\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    261\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# rgb_array\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    265\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    266\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class PolicyIterationAgent:\n",
    "    '''\n",
    "    Класс, эмулирующий работу агента\n",
    "    '''\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Пространство состояний\n",
    "        self.observation_dim = 48\n",
    "        # Массив действий в соответствии с документацией\n",
    "        # https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
    "        self.actions_variants = np.array([0, 1, 2, 3])\n",
    "        # Задание стратегии (политики)\n",
    "        # Карта 4х4 и 6 возможных действий\n",
    "        self.policy_probs = np.full((self.observation_dim, len(self.actions_variants)), 0.25)\n",
    "        # Начальные значения для v(s)\n",
    "        self.state_values = np.zeros(shape=(self.observation_dim))\n",
    "        # Начальные значения параметров\n",
    "        self.maxNumberOfIterations = 1000\n",
    "        self.theta = 1e-6\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def print_policy(self):\n",
    "        '''\n",
    "        Вывод матриц стратегии\n",
    "        '''\n",
    "        print('Стратегия:')\n",
    "        pprint(self.policy_probs)\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        '''\n",
    "        Оценивание стратегии \n",
    "        '''\n",
    "        # Предыдущее значение функции ценности\n",
    "        valueFunctionVector = self.state_values\n",
    "        for iterations in range(self.maxNumberOfIterations):\n",
    "            # Новое значение функции ценности\n",
    "            valueFunctionVectorNextIteration = np.zeros(shape=(self.observation_dim))\n",
    "            # Цикл по состояниям\n",
    "            for state in range(self.observation_dim):\n",
    "                # Вероятности действий\n",
    "                action_probabilities = self.policy_probs[state]\n",
    "                # Цикл по действиям\n",
    "                outerSum = 0\n",
    "                for action, prob in enumerate(action_probabilities):\n",
    "                    innerSum = 0\n",
    "                    # Цикл по вероятностям действий\n",
    "                    for probability, next_state, reward, isTerminalState in self.env.P[state][action]:\n",
    "                        innerSum = innerSum + probability * (reward + self.gamma * self.state_values[next_state])\n",
    "                    outerSum = outerSum + self.policy_probs[state][action] * innerSum\n",
    "                valueFunctionVectorNextIteration[state] = outerSum\n",
    "            if (np.max(np.abs(valueFunctionVectorNextIteration - valueFunctionVector)) < self.theta):\n",
    "                # Проверка сходимости алгоритма\n",
    "                valueFunctionVector = valueFunctionVectorNextIteration\n",
    "                break\n",
    "            valueFunctionVector = valueFunctionVectorNextIteration\n",
    "        return valueFunctionVector\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        '''\n",
    "        Улучшение стратегии \n",
    "        '''\n",
    "        qvaluesMatrix = np.zeros((self.observation_dim, len(self.actions_variants)))\n",
    "        improvedPolicy = np.zeros((self.observation_dim, len(self.actions_variants)))\n",
    "        # Цикл по состояниям\n",
    "        for state in range(self.observation_dim):\n",
    "            for action in range(len(self.actions_variants)):\n",
    "                for probability, next_state, reward, isTerminalState in self.env.P[state][action]:\n",
    "                    qvaluesMatrix[state, action] = qvaluesMatrix[state, action] + probability * (\n",
    "                                reward + self.gamma * self.state_values[next_state])\n",
    "\n",
    "            # Находим лучшие индексы\n",
    "            bestActionIndex = np.where(qvaluesMatrix[state, :] == np.max(qvaluesMatrix[state, :]))\n",
    "            # Обновление стратегии\n",
    "            improvedPolicy[state, bestActionIndex] = 1 / np.size(bestActionIndex)\n",
    "        return improvedPolicy\n",
    "\n",
    "    def policy_iteration(self, cnt):\n",
    "        '''\n",
    "        Основная реализация алгоритма\n",
    "        '''\n",
    "        policy_stable = False\n",
    "        for i in range(1, cnt + 1):\n",
    "            self.state_values = self.policy_evaluation()\n",
    "            self.policy_probs = self.policy_improvement()\n",
    "        print(f'Алгоритм выполнился за {i} шагов.')\n",
    "\n",
    "\n",
    "def play_agent(agent):\n",
    "    env2 = gym.make('CliffWalking-v0', render_mode='human')\n",
    "    state = env2.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        p = agent.policy_probs[state]\n",
    "        if isinstance(p, np.ndarray):\n",
    "            action = np.random.choice(len(agent.actions_variants), p=p)\n",
    "        else:\n",
    "            action = p\n",
    "        next_state, reward, terminated, truncated, _ = env2.step(action)\n",
    "        env2.render()\n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Создание среды\n",
    "    env = gym.make('CliffWalking-v0')\n",
    "    env.reset()\n",
    "    # Обучение агента\n",
    "    agent = PolicyIterationAgent(env)\n",
    "    agent.print_policy()\n",
    "    agent.policy_iteration(1000)\n",
    "    agent.print_policy()\n",
    "    # Проигрывание сцены для обученного агента\n",
    "    play_agent(agent)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
